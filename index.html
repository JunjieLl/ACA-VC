<html>

<head>
    <link rel="stylesheet" type="text/css" href="index.css">
    <script src="index.js"></script>
</head>

<body>
    <div class="outer-outer-container">
        <div class="artical-title"><b>SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention</b>
        </div>
        <div class="author-container">
            <div class="author"><b>Junjie Li</b>, <b>Yiwei Guo</b>, <b>Xie Chen</b>, <b>Kai Yu</b></div>
            <div class="author">MoE Key Lab of Artificial Intelligence, AI Institute</div>
            <div class="author">X-LANCE Lab, Department of Computer Science and Engineering</div>
            <div class="author">Shanghai Jiao Tong University, Shanghai, China</div>
            <div style="font-size: 14px;">{<a href="mailto:junjieli@sjtu.edu.cn">junjieli</a>, <a
                    href="mailto:yiwei.guo@sjtu.edu.cn">yiwei.guo</a>, <a
                    href="mailto:chenxie95@sjtu.edu.cn">chenxie95</a>, <a
                    href="mailto:kai.yu@sjtu.edu.cn">kai.yu</a>}@sjtu.edu.cn</div>
        </div>
        <div class="abstract-container">
            <div class="abstract-inner-container">
                <div class="abstract-title"><b>Abstract</b></div>
                <div class="abstract">
                    Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary
                    unseen target
                    speaker timbre, while keeping the linguistic content unchanged.
                    Although the voice of generated speech can be controlled by providing the speaker embedding of the
                    target
                    speaker, the speaker similarity still lags behind the ground truth recordings.
                    In this paper, we propose SEF-VC, a speaker embedding free voice conversion model,
                    which is designed to learn and incorporate speaker timbre from reference speech via a powerful
                    position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic
                    tokens
                    in a
                    non-autoregressive manner.
                    The concise design of SEF-VC enhances its training stability and voice conversion performance.
                    Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality
                    speech
                    with better similarity to target reference than strong zero-shot VC baselines, even for very short
                    reference
                    speeches.
                </div>
            </div>
        </div>
        <!-- main experiment -->
        <div class="outer-container">
            <div class="spearate-line"></div>
            <div class="exp-title"><b>Performance Comparison Between SEF-VC and Other Baselines in Any-to-Any
                    Voice Conversion</b></div>
            <div class="container">
                <div class="column">
                    <div class="title" style="justify-content: flex-start; ">Conversion Type</div>
                    <div class="text row">Male to Female</div>
                    <div class="text row">Male to Male</div>
                    <div class="text row">Female to Male</div>
                    <div class="text row">Female to Female</div>
                </div>
                <div class="column" id="caption">
                    <div class="title">Transcript</div>
                </div>
                <div class="column" id="src">
                    <div class="title">Source Speech</div>
                </div>
                <div class="column" id="ref">
                    <div class="title">Reference Speech</div>
                </div>
                <div class="column" id="AdaIN_VC">
                    <div class="title"><a href="https://github.com/jjery2243542/adaptive_voice_conversion"
                            target="_blank">AdaIN-VC</a></div>
                </div>
                <div class="column" id="YourTTS">
                    <div class="title"><a href="https://github.com/Edresson/YourTTS" target="_blank">YourTTS</a></div>
                </div>
                <div class="column" id="SSR_VC">
                    <div class="title"><a href="https://github.com/facebookresearch/speech-resynthesis"
                            target="_blank">SSR-VC</a></div>
                </div>
                <div class="column" id="SEF_VC">
                    <div class="title"><b>SEF-VC (Ours)</b></div>
                </div>
            </div>
            <div class="spearate-line"></div>
        </div>
        <!-- prompt length -->
        <div class="outer-container">
            <div class="exp-title"><b>Experiment on The Influence of References of Different Lengths on Zero-Shot
                    VC</b></div>
            <div class="container">
                <div class="column">
                    <div class="title" style="justify-content: flex-start;">Prompt Length</div>
                    <div class="text row"> 2 Seconds</div>
                    <div class="text row"> 3 Seconds</div>
                    <div class="text row"> 5 Seconds</div>
                    <div class="text row">10 Seconds</div>
                </div>
                <div class="column" id="prompt_caption">
                    <div class="title">Transcript</div>
                </div>
                <div class="column" id="prompt_src">
                    <div class="title">Source Speech</div>
                </div>
                <div class="column" id="prompt_ref">
                    <div class="title">Reference Speech</div>
                </div>
                <div class="column" id="prompt_SSR_VC">
                    <div class="title"><a href="https://github.com/facebookresearch/speech-resynthesis"
                            target="_blank">SSR-VC</a></div>
                </div>
                <div class="column" id="prompt_SEF_VC">
                    <div class="title"><b>SEF-VC (Ours)</b></div>
                </div>
            </div>
            <div class="spearate-line"></div>
        </div>
        <!-- spk vc cross -->
        <div class="outer-container">
            <div class="exp-title"><b>Experiment on The Effectiveness of Different Speaker Modeling</b></div>
            <div class="container">
                <div class="column">
                    <div class="title" style="justify-content: flex-start;">Conversion Type</div>
                    <div class="text row">Male to Female</div>
                    <div class="text row">Male to Male</div>
                    <div class="text row">Female to Male</div>
                    <div class="text row">Female to Female</div>
                </div>
                <div class="column" id="spk_caption">
                    <div class="title">Transcript</div>
                </div>
                <div class="column" id="spk_src">
                    <div class="title">Source Speech</div>
                </div>
                <div class="column" id="spk_ref">
                    <div class="title">Reference Speech</div>
                </div>
                <div class="column" id="spk_speaker_embedding">
                    <div class="title">Speaker Embedding</div>
                </div>
                <div class="column" id="spk_cross_attention">
                    <div class="title"><b>SEF-VC (Ours)</b></div>
                </div>
            </div>
            <div class="spearate-line"></div>
        </div>
    </div>
</body>

</html>